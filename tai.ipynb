{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9aHcUnIbDVAO"
   },
   "outputs": [],
   "source": [
    "import torch as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0jSByp3IPogP"
   },
   "outputs": [],
   "source": [
    "from PIL import Image, ImageOps\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "jODDQbBeN547"
   },
   "outputs": [],
   "source": [
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_tetris\n",
    "from gym_tetris.actions import SIMPLE_MOVEMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "id": "ZP8MDZBOfTUC",
    "outputId": "68ae755d-5934-4cb9-af15-3c37d641457c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "env = gym_tetris.make('TetrisA-v0')\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "\n",
    "#a = np.zeros((10000,240, 256, 3), dtype = np.uint8)\n",
    "i = 0\n",
    "done = False\n",
    "env.reset()\n",
    "while not done:\n",
    "    #if done:\n",
    "        #state = env.reset()\n",
    "    state , reward, done, _ = env.step(env.action_space.sample())\n",
    "    #i += 1\n",
    "\n",
    "    #env.render()\n",
    "#print(state.shape)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y3OO3J1ETFLJ"
   },
   "outputs": [],
   "source": [
    "a = np.copy(state[47:209, 95:176, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R1Mgv87sXzyP",
    "outputId": "c29c4aca-11d7-43ea-bccc-3dc0d3ed75c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(162, 81, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QjjEFKc_FaEf"
   },
   "outputs": [],
   "source": [
    "mini_state = (a[..., :3] @ [0.299, 0.587, 0.114]).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b9MPVHImOHZM",
    "outputId": "a749e574-16c7-4d9a-e962-b2646d5c6286"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([130.728,  89.939,  87.29 , ...,  94.929, 121.393, 102.407])"
      ]
     },
     "execution_count": 88,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a[..., :3] @ [0.299, 0.587, 0.114]\n",
    "b[b!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MosoluFvdTGy",
    "outputId": "d2bdf7b3-8d62-44bc-d550-8bb646d0d40e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 111,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mini_state = np.zeros((20,10), dtype=np.int8)\n",
    "idcs_y = np.arange(4,81,8)\n",
    "idcs_x = np.arange(4,162,8)\n",
    "mini_state = mini_state[idcs_x[:, np.newaxis],idcs_y]\n",
    "mini_state[mini_state > 0] = 1\n",
    "mini_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jYX2xTzhXyko",
    "outputId": "2ea8cc62-2621-4d49-c536-0a8c3ddbaad5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  4,  1,\n",
       "        0, 96,  0,  0,  0,  0,  0,  0], dtype=uint8)"
      ]
     },
     "execution_count": 107,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.packbits(mini_state.astype(bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7uq5TSknFsCi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 179
    },
    "id": "8F9b1h-vcHFr",
    "outputId": "18dcb4e2-6dd4-46bf-dfc6-61e0469c60bc"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFEAAACiCAIAAAC267bWAAAAl0lEQVR4nO3VMQ7CMBBEUYNScMQUHJAjptiCGiMRikRWPO+VcbPT5LcGAAAAAAAAAAAAAAAAAAAAAAAAAADwv9uPt6pantV/fT1OPGe8datPbd1G33SA++gDBrA5Q+LmZef5+7/NFelzR5+nYXOGxM36nEGfO/o8DZszJG7W5wz63NHnadicIXGzPgMAAAAAAAAAAABX8gY4h181szaeFwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=81x162 at 0x7F878BA7D1D0>"
      ]
     },
     "execution_count": 112,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im = Image.fromarray(a, 'RGB')\n",
    "#im = im.convert('L')\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q0wsmsGsPgi-"
   },
   "outputs": [],
   "source": [
    "im = Image.fromarray(a, 'L')\n",
    "#im.convert('L')\n",
    "b = np.asarray(im)\n",
    "b = b.astype('bool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 182
    },
    "id": "jItN0kUIbWw_",
    "outputId": "3e6c44b6-67be-4c80-9d3e-07143e0ecbd2"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFoAAAClCAIAAABTDn2ZAAADPElEQVR4nO2d223kMAxFhalgO9wS0kJa2BLSYUrYjwwwJjR0RD2v7HM+lECZ2BRF8lqynaS/KaWURrS78PH5ai9F+xQ+8oMevTUudsqjKZ89r+fPZ0opfaXX99GeR27W9+erbeHLaT2zvAH/y/pbeo79+WeKvF7eoxZNlcnSax7aw/U8mo54Q2qK7tGz1x5xfXtyzGd0DB0xvPAUrsr8ObUgXDt66UiORC0I2vPmuqMXCsOL2jPQHVEUoknIHQrRJOSOKCSLYbNkURhe1B6UhWTxEXKHQjQJuSMKyWLYLFkUhhe1B2UhWXyE3KEQTULuiEKyGDZLFoXhRe1BWUgWHyF3KESTkDuikCyGzZJFYXhRe1AWksVHyB0K0STkjigki2GzZFEYXtQelIVk8RFyh0I0CbkjCsliGB1Nk1B7LNfQ+XAAcFvUSt3Up47zU/Z9A8GTyZz5wvnmMiw/pddT8nrM6Nc78p5jG2XL6xQAEfLajrI8WVtaFJZkgRVtdMU5mjpl8Ubx0wbcoTB75/bU/ZabLIA7DMYdarVgvj0oi+HiyhJVn4srS1R9KKUG3GFAWQwoiwFlAYAq2A1jN8zlUsrSvj/2KL/ZU/JX30bcWIrac+yJHueXw92uR8IImZ433FpZlPcvUJYAI5QFAOpBWVCWJygLygLQCTVlgZ1RjiaiEuC+rP0XBfemvbZfSVke3mLMa/MFz/G1n5JlXsmRS37ro6AtOe+bGwvRdhxq9oBl7e2fc3sW74bN71m8G9a+j3TZ3bBxZazuOKJFdJxZM50LPVC7yhRSlplIKIv3g/Jneq+kLO6apeT6Py9adauPljXLOb1c9gtqwonWzOLOygKWcj95n1GLpnObz48zRFnq2rW7YU2gLHdFrRZMXbPkp1R7A2GmPU92mb3hyvLzpdcaYb6ynMOaBWUZilotYM2yml1mbwNlye/gb6wsve501NFLEdCRWajVApRlNbvM3pbKwrNh3VCzByxqtQBlWc0uszdaWQCgHrVacCNlqbvoGn6ptsvsoSywkL5bchKZ3xe1WnAjZSlhQezsMnsoCyynLj/HZbWQ1qjVApTFgLKgLKALylJkikItmGnPf9P8VGGf+7ztAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=90x165 at 0x7F63260297D0>"
      ]
     },
     "execution_count": 176,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bwuSidiYaDnA",
    "outputId": "fe9f51e5-8d76-48ca-daf8-09318406c079"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([255, 255, 255, ..., 255, 255,   0], dtype=uint8)"
      ]
     },
     "execution_count": 154,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.packbits(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ttuon5ZJSerU",
    "outputId": "7b660092-5ac4-477d-e671-3221fb77a899"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'board_height': 0,\n",
       " 'current_piece': 'Jr',\n",
       " 'next_piece': 'Zh',\n",
       " 'number_of_lines': 0,\n",
       " 'score': 0,\n",
       " 'statistics': {'I': 0, 'J': 1, 'L': 0, 'O': 0, 'S': 0, 'T': 0, 'Z': 0}}"
      ]
     },
     "execution_count": 105,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hytdU7DnMeNe"
   },
   "outputs": [],
   "source": [
    "class QNetworkCNN(nn.Module):\n",
    "    def __init__(self, action_dim):\n",
    "        super(QNetworkCNN, self).__init__()\n",
    "\n",
    "        self.conv_1 = nn.Conv2d(3, 32, kernel_size=8, stride=4)\n",
    "        self.conv_2 = nn.Conv2d(32, 64, kernel_size=4, stride=3)\n",
    "        self.conv_3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc_1 = nn.Linear(8960, 512)\n",
    "        self.fc_2 = nn.Linear(512, action_dim)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        inp = inp.view((1, 3, 210, 160))\n",
    "        x1 = F.relu(self.conv_1(inp))\n",
    "        x1 = F.relu(self.conv_2(x1))\n",
    "        x1 = F.relu(self.conv_3(x1))\n",
    "        x1 = torch.flatten(x1, 1)\n",
    "        x1 = F.leaky_relu(self.fc_1(x1))\n",
    "        x1 = self.fc_2(x1)\n",
    "\n",
    "        return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QR0tEb3rNYHH"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channel, channel, n_res_block, n_res_channel, stride):\n",
    "        super().__init__()\n",
    "\n",
    "        if stride == 4:\n",
    "            blocks = [\n",
    "                nn.Conv2d(in_channel, channel // 2, 4, stride=2, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(channel // 2, channel, 4, stride=2, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(channel, channel, 3, padding=1),\n",
    "            ]\n",
    "\n",
    "        elif stride == 2:\n",
    "            blocks = [\n",
    "                nn.Conv2d(in_channel, channel // 2, 4, stride=2, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(channel // 2, channel, 3, padding=1),\n",
    "            ]\n",
    "\n",
    "        for i in range(n_res_block):\n",
    "            blocks.append(ResBlock(channel, n_res_channel))\n",
    "\n",
    "        blocks.append(nn.ReLU(inplace=True))\n",
    "\n",
    "        self.blocks = nn.Sequential(*blocks)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.blocks(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NPAV3NQgNACm"
   },
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channel, channel):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channel, channel, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channel, in_channel, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        out = self.conv(input)\n",
    "        out += input\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8wxa4ymRNtYl"
   },
   "outputs": [],
   "source": [
    "class QNetworkCNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channel=4,\n",
    "        channel=128,\n",
    "        n_res_block=2,\n",
    "        n_res_channel=32,\n",
    "        stride=2,\n",
    "        action_dim=4\n",
    "        ):\n",
    "      \n",
    "        self.enc = Encoder(in_channel, channel, n_res_block, n_res_channel, stride)\n",
    "        self.fc_1 = nn.Linear(8960, 512)\n",
    "        self.fc_2 = nn.Linear(512, action_dim)\n",
    "      \n",
    "\n",
    "    def forward(self, inp):\n",
    "      inp = inp.view((1, 163, 80))\n",
    "      x = self.enc.forward(inp)\n",
    "      x = torch.flatten(x, 1)\n",
    "      x = F.relu(self.fc_1(x))\n",
    "      x = T.nn.Softmax(self.fc_2(x))\n",
    "\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DVoJ-okeSQLF"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size, input_shape):\n",
    "        self.size_ctr = 0\n",
    "        self.max_size = max_size\n",
    "\n",
    "        self.states = np.zeros((max_size, *input_shape), dtype=np.float32) #TODO: uint8\n",
    "        self.rewards = np.zeros(max_size, dtype=np.float32)\n",
    "        self.actions = np.zeros(max_size, dtype=np.float32)\n",
    "        self.is_dones = np.zeros(max_size, dtype=np.float32)\n",
    "\n",
    "\n",
    "    def store(self, state, action, reward, done):\n",
    "        idx = self.size_ctr % self.max_size\n",
    "\n",
    "        if not done:\n",
    "          self.states[self.size_ctr] = state\n",
    "          \n",
    "        self.actions[idx] = action\n",
    "        self.rewards[idx] = reward\n",
    "        self.is_dones[idx] = done\n",
    "        self.size_ctr += 1\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "\n",
    "        max_idx = min(self.max_size, self.size_ctr) - 1\n",
    "        idcs = np.random.choice(max_idx, batch_size, replace=False)\n",
    "\n",
    "        states = self.state[idcs]\n",
    "        next_states = self.states[idcs+1]\n",
    "        actions = self.actions[idcs]\n",
    "        rewards = self.rewards[idcs]\n",
    "        is_dones = self.is_done[idcs]\n",
    "\n",
    "        return states, next_states, actions, rewards, is_dones\n",
    "    \n",
    "    def getData(self):\n",
    "        return np.array(self.states[self.size_ctr], self.rewards[self.size_ctr],\n",
    "                        self.actions[self.size_ctr], self.is_dones[self.size_ctr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MnA6ma9EjtHJ"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, batch_size, input_shape, max_buffer_size, epsilon, gamma, lr, epsilon_decay\n",
    "                 , min_epsilon, n_actions, training=True):\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.max_buffer_size = max_buffer_size\n",
    "            \n",
    "        self.target_QNetwork = QNetworkCNN(action_dim=n_actions).to(device)\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        self.replay_buffer = ReplayBuffer(max_buffer_size, input_shape)\n",
    "\n",
    "        for parameter in target_QNetwork.parameters():\n",
    "            parameter.requires_grad = False\n",
    "\n",
    "        if training:\n",
    "            self.lr = lr\n",
    "            self.batch_size = batch_size\n",
    "            \n",
    "            self.predict_QNetwork = QNetworkCNN(action_dim=n_actions).to(device)\n",
    "            self.target_QNetwork.load_state_dict(self.predict_QNetwork.state_dict())\n",
    "            \n",
    "            self.optimizer = T.optim.Adam(self.predict_QNetwork.parameters(), lr=lr)\n",
    "            #self.scheduler = StepLR(optimizer, step_size=lr_step, gamma=lr_gamma)\n",
    "\n",
    "            \n",
    "\n",
    "    \n",
    "    def chooseAction(self, state, use_epsilon=True):\n",
    "\n",
    "      if not use_epsilon or np.random.random() > self.epsilon:\n",
    "        state = torch.Tensor(state).to(device) #TODO\n",
    "        with torch.no_grad():\n",
    "            predictions = self.predict_QNetwork(state)\n",
    "        return T.argmax(predictions) #TODO\n",
    "        \n",
    "      else:\n",
    "        return np.random.randint(self.n_actions)\n",
    "        \n",
    "    def train(self, epochs):\n",
    "\n",
    "      for _ in range(epochs):\n",
    "\n",
    "        states, next_states, actions, rewards, is_done = self.replay_buffer.sample(batch_size)\n",
    "\n",
    "        Q_values = self.predict_QNetwork(states)\n",
    "\n",
    "        next_Q_values = self.predict_QNetwork(next_states)\n",
    "        next_Q_state_values = self.target_QNetwork(next_states)\n",
    "\n",
    "        Q_value = Q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        next_q_value = next_q_state_values.gather(1, T.max(next_q_values, 1)[1].unsqueeze(1)).squeeze(1)\n",
    "        expected_q_value = rewards + self.gamma * next_q_value * (1 - is_done)\n",
    "\n",
    "        loss = T.nn.MSELoss(q_value, expected_q_value.detach()) #no gradients needed for expected values\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()          #TODO: crossEntropy ?\n",
    "\n",
    "        T.save()\n",
    "\n",
    "    def preFillReplayBuffer(self, n_steps):\n",
    "\n",
    "      for _ in range(n_steps):\n",
    "        action = chooseAction(env, state, eps)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "\n",
    "        self.replay_buffer.store(state, action, reward, done)\n",
    "\n",
    "#TODO: put replayBuffer in VRAM\n",
    "\n",
    "    def previewAgent(self, weights_file=None):\n",
    "      if weights_file:\n",
    "        #T.load\n",
    "        pass\n",
    "\n",
    "      total_reward = 0\n",
    "      state = env.reset()\n",
    "      done = False\n",
    "      while not done:\n",
    "          state = T.Tensor(state).to(device)\n",
    "          with torch.no_grad():\n",
    "              values = self.chooseAction(state)\n",
    "      \n",
    "          state, reward, done, _ = env.step(action)\n",
    "          total_reward += reward\n",
    "          env.render()\n",
    "          \n",
    "    \n",
    "    return total_reward\n",
    "\n",
    "    #def loadWeights(self, file_path):\n",
    "       # self.targetQNetwork\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_tetris\n",
    "from gym_tetris.actions import SIMPLE_MOVEMENT\n",
    "from AgentDoubleQ import Agent\n",
    "\n",
    "class GameCollector:\n",
    "    \n",
    "    def __init__(self, weights_file):\n",
    "        self.agent = Agent(training=False, max_buffer_size=10000)\n",
    "        self.weights_file = weights_file\n",
    "        \n",
    "        self.tmp_states = np.zeros((4, *self.input_shape), dtype=np.uint8) \n",
    "        self.tmp_rewards = np.zeros(4, dtype=np.uint8)\n",
    "        self.tmp_actions = np.zeros(4, dtype=np.uint8)\n",
    "        self.tmp_is_dones = np.zeros(4, dtype=np.uint8)\n",
    "        \n",
    "    def preProcessFrame(self, frame):\n",
    "        frame = np.copy(frame[47:209, 95:176, :])\n",
    "        frame = (frame[..., :3] @ [0.299, 0.587, 0.114]).astype(np.uint8)\n",
    "        idcs_y = np.arange(4,81,8)\n",
    "        idcs_x = np.arange(4,162,8)\n",
    "        frame = frame[idcs_x[:, np.newaxis],idcs_y]\n",
    "        \n",
    "        return frame\n",
    "    \n",
    "    def play(self, epochs, steps_limit):\n",
    "        env = gym_tetris.make('TetrisA-v0')\n",
    "        env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "        \n",
    "        for _ in range(epochs):\n",
    "            env.reset()\n",
    "            done = False\n",
    "            steps_ctr = 0\n",
    "            \n",
    "            while not done:\n",
    "                idx = steps_ctr % 4\n",
    "                action = self.chooseAction()\n",
    "                state, reward, done, _ = env.step(action)\n",
    "                \n",
    "                tmp_states[idx] =  self.preProcessFrame(state)\n",
    "                tmp_rewards[idx] = reward\n",
    "                tmp_actions[idx] = action\n",
    "                tmp_is_dones[idx] = done\n",
    "                \n",
    "                \n",
    "                if step_ctr and idx == 0:\n",
    "                    self.agent.replay_buffer.store(tmp_states, tmp_actions, tmp_rewards, tmp_is_dones)\n",
    "                \n",
    "                steps_ctr += 1\n",
    "                   \n",
    "        env.close()\n",
    "    \n",
    "    def run(self):\n",
    "    \n",
    "        self.target_QNetwork.load_state_dict(self.weights_file)\n",
    "        self.play(10)\n",
    "        np.save('test', self.agent.replay_buffer.getData())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
